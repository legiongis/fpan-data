### Creating Thesauri and Concept Collections From CSVs

##### Step 1

create CSV files in a dedicated directory, per thesaurus (i.e. if you have two separate sets of the CSVs from which you want to make two separate thesauri, place each set in its own directory). note that no header rows were used in the development of the following scripts, but untested support for header rows has been attempted.

##### Step 2

add UUIDs to the CSV files

+ run command
        python add_uuid_to_csvs.py <csv directory> [-s/--safe]
        
will add a new column to the csv files and populate it with new UUIDs. if this column exists already, no new column will be added and the existing values will remain unchanged. currently the testing for this happens on a per-column basis, which means that if a new value is added to the end of a CSV that already has UUIDs in it, no new UUID will be added to that row, as the entire file will be skipped when the column first encountered (this could be improved...).

add the `-s/--safe` boolean argument to place the resulting files into a new directory, instead of overwriting the original files.
        
##### Step 3

create thesaurus from each directory full of CSVs. at the same time, a collections file is also made. each csv is treated as a top concept with descendent concepts. accordingly, a collection is created for each csv and all of the concepts that are generated by the csv are added to that collection. thus, the result is really two files, a thesaurus file and a collections file, named <csv directory>-thesaurus.xml and <csv directory>-collections.xml.

+ run command
        python thesaurus_from_csvs.py <csv directory> [-m/--mock]
        
will process the input directory and create a thesaurus and collections file, as described above. add the `-m/--mock` argument to create mock UUIDs throughout the whole new thesaurus. this may be useful for debugging, but does not created a valid thesaurus.

Note that not all of the options have properly been moved to argparse yet, so you will need to edit some stuff within the script itself. 

+ you can specify which files have contents that should be sorted alphabetically (otherwise a sortorder is added to concepts to recreate their original order in the csv files).
+ the output directory is generated by the script, and you may need to edit it manually.
+ you can change what column the script will use for the new preflabels for the concepts, and which column holds the UUIDs (this must be the same for all csvs, however you can use index references, so `:-1` will specify the last column in each csv, regardless of the number of columns therein.

very importantly, if the thesaurus/collections files exist before the script is made, then the topconcept uuids and collections uuids will be retrieved from them instead of being regenerated. this is necessary as resources models may reference collection ids so those must remain constant, and if new topconceptids are created, you will double all of your existing top concepts, even if you load the concepts with `-ow overwrite`.

### Quarterly Updates to HMS from FMSF

many improvements possible... these are steps as of 10-30-17

#### Inputs

3 shapefiles from FMSF: Historic Structures, Historic Cemeteries, Archaeological Sites
    These shapefiles come straight from Chip Birdsong at FMSF.
1 csv from FMSF: OwnType
    This actually comes as an Excel file from Chip, so a quick open in Excel/save to csv must happen.
1 shapefile for spatial filtering of Historic Structures
    This has been created ahead of time.

##### Step 1

filter historic structures based on predetermined criteria

+ spatial filter applied based on premade shapefile
+ attribute filter based on whether the structure has been destroyed
+ attribute filter based on whether the structure is a lighthouse

+ run command
        python filter_structures.py HistoricStructures.shp --clip clip-shapefile\struct_filter.shp

will produce a filtered shapefile inside of a directory called processed_<today's date>

##### Step 2

manually copy the Historic Cemetery and Archaeological Sites shapefile into this new processed_<today's date> directory.

##### Step 3

add ownership values to each shapefile in this new directory

+ run command
        python add_ownership_values.py processed_<today's date> OwnType.csv

will modify each shapefile in place, adding a new column called OWNERSHIP if necessary, and copying all of the OwnType values to the appropriate rows.

##### Step 4

+ create csv files from the shapefiles. during this process a fair bit of data sanitization is performed as well.
    + remove "0" from the values (this is a dataset specific need, may not be a good general rule)
    + allow for the pre-determination of date fields, and some basic data parsing and cleanup is performed on values in those fields.

+ run command
        python convert_shp_to_csv.py process_<today's date> [-t int]

will create csv files for each shapefile in the directory. the option -t/--truncate argument can be added to create a smaller version of the csvs for testing. enter -t 10 to process only the first 10 features in each shapefile.

##### Step 5

loading the data can be one file at a time or with the load_package command if the csvs are moved to the appropriate location in the package
